{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a1f5b9",
   "metadata": {},
   "source": [
    "<font size ='3'>*First, let's read in the data and necessary libraries*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8f26d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mypy import print_side_by_side\n",
    "from mypy import display_side_by_side\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cbce64f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cal = pd.read_csv('boston_calendar.csv')\n",
    "s_cal = pd.read_csv('seatle_calendar.csv')\n",
    "b_list = pd.read_csv('boston_listings.csv')\n",
    "s_list = pd.read_csv('seatle_listings.csv')\n",
    "b_rev = pd.read_csv('boston_reviews.csv')\n",
    "s_rev = pd.read_csv('seatle_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7e183",
   "metadata": {},
   "source": [
    " _______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51636c9f",
   "metadata": {},
   "source": [
    "## Task 1: Business Understanding <font size=\"2\"> *(With some Data Preperation)*</font>  \n",
    "<font size=\"3\"> *My work flow will be as follows, I will explore the data with some cleaning to get enough insights to formulate questions, then, within every question I will follow the rest of the steps of the CRISP-DM framework.*</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d211d",
   "metadata": {},
   "source": [
    "### Step 1: Basic Exploration with some cleaning\n",
    "<font size ='3'>*To be familiarized with the Data and to gather insights to formulate questions*<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28402b3d",
   "metadata": {},
   "source": [
    "> **Boston & Seatle Calendar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bd12a30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>b_cal</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-03</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-02</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>s_cal</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>241032</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>t</td>\n",
       "      <td>$85.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>241032</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>t</td>\n",
       "      <td>$85.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241032</td>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>241032</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>241032</td>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_side_by_side(b_cal.head(), s_cal.head(), titles = ['b_cal', 's_cal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffdc6d3",
   "metadata": {},
   "source": [
    "<font size ='3'>*Check the sizes of cols and rows & check Nulls*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e7696f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston Cal:                                              Seatle Cal:\n",
      "Shape:  (1308890 4)                                      Shape:  (1393570 4)\n",
      "Cols with nulls:   price                                 Cols with nulls:   price\n",
      "Null prop of price column:   0.51                        Null prop of price column:   0.33\n",
      "Proportion of False(unit unavailable):  0.51             Proportion of False(unit unavailable):  0.33\n",
      "Nulls when units are available:   0                      Nulls when units are available:   0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_side_by_side('Boston Cal:', 'Seatle Cal:', b=0)\n",
    "print_side_by_side('Shape:',b_cal.shape,\"Shape:\",  s_cal.shape)\n",
    "print_side_by_side(\"Cols with nulls: \", b_cal.isnull().sum()[b_cal.isnull().sum()>0].index[0],\"Cols with nulls: \", s_cal.isnull().sum()[s_cal.isnull().sum()>0].index[0])\n",
    "print_side_by_side(\"Null prop of price column: \", round(b_cal.price.isnull().sum()/b_cal.shape[0], 2),\"Null prop of price column: \", round(s_cal.price.isnull().sum()/s_cal.shape[0], 2))\n",
    "print_side_by_side(\"Proportion of False(unit unavailable):\", round(b_cal.available[b_cal.available =='f' ].count()/b_cal.shape[0],2),\"Proportion of False(unit unavailable):\", round(s_cal.available[s_cal.available =='f' ].count()/s_cal.shape[0],2))\n",
    "print_side_by_side(\"Nulls when units are available: \", b_cal[b_cal['available']== 't']['price'].isnull().sum(),\"Nulls when units are available: \", s_cal[s_cal['available']== 't']['price'].isnull().sum() )\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca1fe2",
   "metadata": {},
   "source": [
    "<font size ='3'>*Let's do some cleaning, first, let's transfer `date` column to datetime to ease manipulation and analysis. I will also create a dataframe with seperate date items from the Date column, to check the time interval along which the data was collected. In addition to that, let's transform `price` and `available` into numerical values*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "121e4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dateparts(df, date_col): \n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    date_col -list of columns to break down into columns of years,months and days.\n",
    "    \n",
    "    OUTPUT\n",
    "    df - a dataframe with columns of choice transformed in to columns of date parts(years,months and days)\n",
    "    \"\"\"\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "    b_date_df = pd.DataFrame()\n",
    "    b_date_df['year'] = df['date'].dt.year\n",
    "    b_date_df['month'] = df['date'].dt.month\n",
    "    b_date_df['day'] =df['date'].dt.strftime(\"%A\")\n",
    "    #b_date_df['dow'] =df['date'].dt.day\n",
    "    df = df.join(b_date_df)\n",
    "    return df\n",
    "#######################\n",
    "def get_period_df(df):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    \n",
    "    OUTPUT\n",
    "    df - a dataframe grouped to show the span of all the entries\n",
    "    \"\"\"\n",
    "    period =pd.DataFrame(df.groupby(['year','month'], sort = True)['day'].value_counts())\n",
    "    period = period.rename(columns={'day':'count'}, level=0)\n",
    "    period = period.reset_index().sort_values(by=['year', 'month', 'day']).reset_index(drop = True)\n",
    "    return period\n",
    "#############################\n",
    "def to_float(df, float_cols):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    float_cols -list of columns to transform to float\n",
    "    \n",
    "    OUTPUT\n",
    "    df - a dataframe with columns of choice transformed to float \n",
    "    \"\"\"\n",
    "    for col in float_cols:\n",
    "            df[col] = df[col].str.replace('$', \"\", regex = False)\n",
    "            df[col] = df[col].str.replace('%', \"\", regex = False)\n",
    "            df[col] = df[col].str.replace(',', \"\", regex = False)\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df\n",
    "#############################\n",
    "def bool_nums(df, bool_cols):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    bool_cols -list of columns with true or false strings\n",
    "    \n",
    "    OUTPUT\n",
    "    df - a dataframe with columns of choice transforemed into binary values\n",
    "    \"\"\"\n",
    "    for col in bool_cols:\n",
    "        df[col] = df[col].apply(lambda x: 1 if x == 't' else 0 )\n",
    "    df = df.reset_index(drop= True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a79c74",
   "metadata": {},
   "source": [
    "<font size = '3'>*Let's take a look at the resulted DataFrames after executing the previous fuc=nctions. I flipped the Boston calendar to have it start in ascending order like Seatle.*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "88b18f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>b_cal_1</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2016-09-06</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2016-09-07</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2016-09-08</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>s_cal_1</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>241032</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>241032</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241032</td>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b_cal_1 = to_float(b_cal, ['price'])\n",
    "s_cal_1 = to_float(s_cal, ['price'])\n",
    "b_cal_1 = create_dateparts(b_cal_1, 'date')\n",
    "s_cal_1 = create_dateparts(s_cal_1, 'date')\n",
    "b_cal_1 = bool_nums(b_cal_1, ['available'])\n",
    "s_cal_1 = bool_nums(s_cal_1, ['available'])\n",
    "b_cal_1 = b_cal_1.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "display_side_by_side(b_cal_1.head(3),s_cal_1.head(3), titles = ['b_cal_1', 's_cal_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075f145",
   "metadata": {},
   "source": [
    "<font size = '3'>*Let's take a look at the resulted time intervals for Both Boston and Seatle calendar tables*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0b367008",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>Boston Period</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>Friday</td>\n",
       "      <td>14344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2></br></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>3586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>Seatle Period</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>15272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2></br></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>3818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Listing IDs in Boston Calendar:  3585\n",
      "Number of unique Listing IDs in Seatle Calendar:  3818\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b_period =get_period_df(b_cal_1)\n",
    "s_period =get_period_df(s_cal_1)\n",
    "display_side_by_side(b_period.head(1), b_period.tail(1), titles = ['Boston Period'])\n",
    "display_side_by_side(s_period.head(1), s_period.tail(1), titles = ['Seatle Period'])\n",
    "\n",
    "print(\"Number of unique Listing IDs in Boston Calendar: \", len(b_cal_1.listing_id.unique()))\n",
    "print(\"Number of unique Listing IDs in Seatle Calendar: \", len(s_cal_1.listing_id.unique()))\n",
    "print('\\n')\n",
    "#b_period.iloc[0], s_period.iloc[0], b =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48c774",
   "metadata": {},
   "source": [
    "<font size ='3'>*Seems like they both span a year, through which all the listings are tracked in terms of availability. When we group by year and month; the count is equivalent to the numbers of the unique ids because all the ids are spanning the same interval.  Let's check any anomalies*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "032984ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anomalies(df, col):\n",
    "    list_ids_not_year_long = []\n",
    "    for i in sorted(list(df[col].unique())):\n",
    "        if df[df[col]== i].shape[0] != 365:\n",
    "            list_ids_not_year_long.append(i)\n",
    "    print(\"Entry Ids that don't span 1 year: \" , list_ids_not_year_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c5f1b110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry Ids that don't span 1 year:  [12898806]\n"
     ]
    }
   ],
   "source": [
    "#Boston\n",
    "check_anomalies(b_cal_1, 'listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "921e74b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry Ids that don't span 1 year:  []\n"
     ]
    }
   ],
   "source": [
    "#Seatle\n",
    "check_anomalies(s_cal_1, 'listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2cfcd932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span of the entries for this listing, should be 365:  730\n",
      "Should be 1:  8\n",
      "Size of anomaly listing, Should be = 365:  365\n",
      "After removing duplicates, Span of the entries for this listing, should be 365:  365\n",
      "After removing duplicates, shape is:  (1308525, 7)\n"
     ]
    }
   ],
   "source": [
    "## check this entry in Boston Calendar\n",
    "print(\"Span of the entries for this listing, should be 365: \", b_cal_1[b_cal_1['listing_id']== 12898806].shape[0])\n",
    "## 2 years, seems like a duplicate as 730 = 365 * 2\n",
    "one_or_two = pd.DataFrame(b_cal_1[b_cal_1['listing_id']==12898806].groupby(['year', 'month', 'day'])['day'].count()).day.unique()[0]\n",
    "print(\"Should be 1: \", one_or_two)\n",
    "## It indeed is :)\n",
    "b_cal_1 = b_cal_1.drop_duplicates()\n",
    "print(\"Size of anomaly listing, Should be = 365: \", b_cal_1.drop_duplicates()[b_cal_1.drop_duplicates().listing_id==12898806]['listing_id'].size)\n",
    "print(\"After removing duplicates, Span of the entries for this listing, should be 365: \", b_cal_1[b_cal_1['listing_id']== 12898806].shape[0])\n",
    "print(\"After removing duplicates, shape is: \", b_cal_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bec4ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_cal_1.to_csv('b_cal_1.csv')\n",
    "# s_cal_1.to_csv('s_cal_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c28022",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________\n",
    "### Comments:  \n",
    "[Boston & Seatle Calendar]\n",
    "- The datasets have information about listing dates, availability and price tracked over a year for ever listing id\n",
    "- There are no data entry errors, all nulls are due to the structuring of the Data (the listings that weren't available has no price)\n",
    "- I added 4 cols that contain dateparts that will aid further analysis and modeling\n",
    "- The Boston calendar Dataset ranges through `365`days from `6th of September'16` to `5th of September'17`, No nulls with `1308525` rows and  `8` cols\n",
    "- The Seatle calendar Dataset ranges through `365`days from `4th of January'16` to `2nd of January'17`, No nulls with `1393570` rows and  `8` cols\n",
    "- Number of unique Listing IDs in Boston Calendar:  `3585`\n",
    "- Number of unique Listing IDs in Seatle Calendar:  `3818`\n",
    "- It seems that the table is not documenting any rentals it just shows if the unit is available at a certain time and the price then."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b224bb",
   "metadata": {},
   "source": [
    " _______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe6e13",
   "metadata": {},
   "source": [
    "## Step 1: Continue - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940cb2e1",
   "metadata": {},
   "source": [
    "> **Boston & Seatle Listings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5d1d6a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>listing_url</th>\n",
       "      <th>scrape_id</th>\n",
       "      <th>last_scraped</th>\n",
       "      <th>name</th>\n",
       "      <th>summary</th>\n",
       "      <th>space</th>\n",
       "      <th>description</th>\n",
       "      <th>experiences_offered</th>\n",
       "      <th>neighborhood_overview</th>\n",
       "      <th>...</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>requires_license</th>\n",
       "      <th>license</th>\n",
       "      <th>jurisdiction_names</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>require_guest_profile_picture</th>\n",
       "      <th>require_guest_phone_verification</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>reviews_per_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12147973</td>\n",
       "      <td>https://www.airbnb.com/rooms/12147973</td>\n",
       "      <td>20160906204935</td>\n",
       "      <td>2016-09-07</td>\n",
       "      <td>Sunny Bungalow in the City</td>\n",
       "      <td>Cozy, sunny, family home.  Master bedroom high...</td>\n",
       "      <td>The house has an open and cozy feel at the sam...</td>\n",
       "      <td>Cozy, sunny, family home.  Master bedroom high...</td>\n",
       "      <td>none</td>\n",
       "      <td>Roslindale is quiet, convenient and friendly. ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>moderate</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                            listing_url       scrape_id  \\\n",
       "0  12147973  https://www.airbnb.com/rooms/12147973  20160906204935   \n",
       "\n",
       "  last_scraped                        name  \\\n",
       "0   2016-09-07  Sunny Bungalow in the City   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Cozy, sunny, family home.  Master bedroom high...   \n",
       "\n",
       "                                               space  \\\n",
       "0  The house has an open and cozy feel at the sam...   \n",
       "\n",
       "                                         description experiences_offered  \\\n",
       "0  Cozy, sunny, family home.  Master bedroom high...                none   \n",
       "\n",
       "                               neighborhood_overview  ... review_scores_value  \\\n",
       "0  Roslindale is quiet, convenient and friendly. ...  ...                 NaN   \n",
       "\n",
       "  requires_license license jurisdiction_names instant_bookable  \\\n",
       "0                f     NaN                NaN                f   \n",
       "\n",
       "  cancellation_policy require_guest_profile_picture  \\\n",
       "0            moderate                             f   \n",
       "\n",
       "  require_guest_phone_verification calculated_host_listings_count  \\\n",
       "0                                f                              1   \n",
       "\n",
       "   reviews_per_month  \n",
       "0                NaN  \n",
       "\n",
       "[1 rows x 95 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_list.head(1)\n",
    "#s_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc69a78",
   "metadata": {},
   "source": [
    " <font size ='3'>*Check the sizes of cols & rows & check Nulls*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5192d561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston listings size :  (3585 95)                        Seatle listings size :  (3818 92)\n",
      "Number of Non-null cols in Boston listings:   51         Number of Non-null cols in Seatle listings:   47\n",
      "Columns in Boston but not in Seatle:   {'interaction', 'house_rules', 'access'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_side_by_side(\"Boston listings size :\", b_list.shape, \"Seatle listings size :\", s_list.shape)\n",
    "print_side_by_side(\"Number of Non-null cols in Boston listings: \",  np.sum(b_list.isnull().sum()==0) ,\"Number of Non-null cols in Seatle listings: \",  np.sum(s_list.isnull().sum()==0))\n",
    "set_difference = set(b_list.columns) - set(s_list.columns)\n",
    "print(\"Columns in Boston but not in Seatle:  \", set_difference)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d0dc1",
   "metadata": {},
   "source": [
    " <font size ='3'>*Let's go through the columns of this table as they are a lot, decide on which would be useful, which would be ignored and which would be transformed based on intuition.* <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e977272",
   "metadata": {},
   "source": [
    "> **to_parts:**<br><font size = '2'>(Divide into ranges)<font/><br>\n",
    ">* *maximum_nights*  \n",
    "><br>  \n",
    ">   \n",
    "> **to_count:** <br><font size = '2'>(Provide a count)<font/><br>\n",
    "> * *amenities*              <br>\n",
    "> * *host_verifications*    \n",
    "><br>  \n",
    ">    \n",
    ">**to_dummy:** <br><font size = '2'>(Convert into dummy variables)<font/><br>\n",
    ">* *amenities*        \n",
    "><br>   \n",
    ">   \n",
    ">**to_len_text:** <br><font size = '2'>(provide length of text)<font/><br>\n",
    ">* *name*                   \n",
    ">* *host_about*            \n",
    ">* *summary*                 \n",
    ">* *description*                            \n",
    ">* *neighborhood_overview*    \n",
    ">* *transit* \n",
    "><br>   \n",
    ">\n",
    ">**to_days:** <br><font size = '2'>(calculate the difference between both columns to have a meaningful value of host_since in days)<font/><br>\n",
    ">* *host_since*\n",
    ">* *last_review*\n",
    "><br>   \n",
    ">\n",
    ">**to_float:**<br><font size = '2'>(Transform to float)<font/><br>\n",
    ">* *cleaning_fee*                   <br>\n",
    ">* *host_response_rate*             <br>\n",
    ">* *host_acceptance_rate*           <br>\n",
    ">* *host_response_rate*             <br> \n",
    ">* *host_acceptance_rate*           <br>\n",
    ">* *extra_people*                   <br>\n",
    ">* *price*                          <br>\n",
    "><br>  \n",
    ">\n",
    "> **to_binary:** <br><font size = '2'>(Transform to binary)<font/><br>\n",
    ">* *host_has_profile_pic*                \n",
    ">* *host_identity_verified*                 \n",
    ">* *host_is_superhost*            \n",
    ">* *is_location_exact*                 \n",
    ">* *instant_bookable*                       \n",
    ">* *require_guest_profile_picture*          \n",
    ">* *require_guest_phone_verification*      \n",
    "><br>   \n",
    ">\n",
    ">**to_drop:**<br><font size = '2'>(Columns to be dropped)<font/>\n",
    "<br><br>\n",
    ">**reason:  little use:** <br> \n",
    ">* *listing_url, scrape_id, last_scraped, experiences_offered, thumbnail_url,xl_picture_url, medium_url,*\n",
    ">* *host_id, host_url, host_thumbnail_url, host_picture_url, host_total_listings_count, neighbourhood,* \n",
    ">* *neighbourhood_group_cleansed, state, country_code, country, latitude, longitude,*\n",
    ">* *has_availability, calendar_last_scraped, host_name, picture_url, space, first_review, *\n",
    "><br><br>\n",
    ">   \n",
    ">**reason:  Nulls, text, only in Boston:** <br>\n",
    ">* *access , interaction, house_rules*\n",
    "><br><br>\n",
    ">\n",
    ">**reason>  Nulls, 0 variability or extreme variability:**      <br>\n",
    ">* *square_feet* ------------- *90% Null boston  97% Null seatle* <br>\n",
    ">* *weekly_price*-------------*75% Null boston  47% Null seatle*  <br>\n",
    ">* *monthly_price*------------*75% Null boston  60% Null seatle*  <br>\n",
    ">* *security_deposit*---------*65% Null boston  51% Null seatle*  <br>\n",
    ">* *notes*---------------------*55% Null boston  42% Null seatle* <br>\n",
    ">* *jurisdiction_names*---------*100% Null in both*               <br>\n",
    ">* *license*--------------------*100% Null in both*                                \n",
    ">* *required_license*-----------*100% Null in both*               <br>\n",
    ">* *street*---------------------*High variability*                <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e75f47",
   "metadata": {},
   "source": [
    "<font size = '3' >*Let's write anymore functions needed to carry on these suggested changes*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ac8752b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['listing_url', 'scrape_id', 'last_scraped', 'experiences_offered', 'thumbnail_url','xl_picture_url', \n",
    "'medium_url', 'host_id', 'host_url', 'host_thumbnail_url', 'host_picture_url', 'host_total_listings_count', \n",
    "'neighbourhood', 'neighbourhood_group_cleansed','state', 'country_code', 'country', 'latitude', 'longitude', \n",
    "'has_availability', 'calendar_last_scraped', 'host_name','square_feet', \n",
    "'weekly_price', 'monthly_price', 'security_deposit', 'notes', 'jurisdiction_names', 'license', 'requires_license', \n",
    "'street', 'picture_url', 'space','first_review', 'house_rules', 'access', 'interaction']\n",
    "float_cols = ['cleaning_fee', 'host_response_rate','host_acceptance_rate','host_response_rate',\n",
    "              'host_acceptance_rate','extra_people','price']\n",
    "len_text_cols = ['name', 'host_about', 'summary', 'description','neighborhood_overview', 'transit']\n",
    "count_cols =  ['amenities', 'host_verifications'] \n",
    "d_col = [ 'amenities']\n",
    "part_col = ['maximum_nights']\n",
    "bool_cols = ['host_has_profile_pic', 'host_identity_verified', 'host_is_superhost', 'is_location_exact',\n",
    "             'instant_bookable', 'require_guest_profile_picture' , 'require_guest_phone_verification' ]  \n",
    "day_cols = [ 'host_since', 'last_review']\n",
    "###########################################################################################################################\n",
    "def to_drop(df, drop_cols):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    drop_cols -list of columns to drop\n",
    "    \n",
    "    OUTPUT\n",
    "    df - a dataframe with columns of choice dropped \n",
    "    \"\"\"\n",
    "    for col in drop_cols:\n",
    "        if col in list(df.columns):\n",
    "            df = df.drop(col, axis = 1)\n",
    "        else:\n",
    "            continue\n",
    "    return df\n",
    "#################################\n",
    "def to_len_text(df, len_text_cols):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    len_text_cols- list of columns to return the length of text of their values\n",
    "    \n",
    "    OUTPUT\n",
    "    df - a dataframe with columns of choice transformed to len(values) instead of long text\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    len_text = []\n",
    "    new_len_text_cols = [] \n",
    "\n",
    "    for col in len_text_cols:\n",
    "        new_len_text_cols.append(\"len_\"+col)\n",
    "\n",
    "        for i in df_new[col]:\n",
    "            #print(col,i)\n",
    "            try:\n",
    "                len_text.append(len(i))\n",
    "            except:\n",
    "                len_text.append(i)\n",
    "        #print('\\n'*10)   \n",
    "        df_new = df_new.drop(col, axis = 1)\n",
    "        len_text_col = pd.Series(len_text)  \n",
    "        len_text_col = len_text_col.reset_index(drop = True)\n",
    "        #print(len_text_col)\n",
    "        df_new['len_'+col]= len_text_col\n",
    "        len_text = []\n",
    "        df_new[new_len_text_cols] = df_new[new_len_text_cols].fillna(0)\n",
    "    return df_new, new_len_text_cols\n",
    "#########################\n",
    "def to_parts(df, part_col):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    part_col -list of columns to divide into \"week or less\" and \"more than a week\" depending on values\n",
    "    \n",
    "    OUTPUT\n",
    "    df - a dataframe with columns of choice transformed to ranges of \"week or less\" and \"more than a week\"\n",
    "    \"\"\"\n",
    "    def to_apply(val):\n",
    "        if val <= 7:\n",
    "            val = '1 Week or less'\n",
    "        elif (val >7) and (val<=14):\n",
    "            val = '1 week to 2 weeks'\n",
    "        elif (val >14) and (val<=30):\n",
    "            val = '2 weeks to 1 month'\n",
    "        elif (val >30) and (val>=60):\n",
    "            val = '1 month to 2 months'\n",
    "        elif (val >60) and (val>=90):\n",
    "            val = '2 month to 3 months'\n",
    "        elif (val >90) and (val>=180):\n",
    "            val = '3 month to 6 months'\n",
    "        else:\n",
    "            val = 'More than 6 months'       \n",
    "        return val\n",
    "    for part in part_col:\n",
    "        df[part]= df[part].apply(to_apply)\n",
    "    return df\n",
    "############################\n",
    "def to_count(df, count_cols): \n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    count_cols -list of columns to count the string items within each value\n",
    "    \n",
    "    OUTPUT\n",
    "    df - a dataframe with columns of choice transformed to a count of values  \n",
    "    \"\"\"\n",
    "    def to_apply(val):\n",
    "        if \"{\" in val:\n",
    "            val = val.replace('}', \"\").replace('{', \"\").replace(\"'\",\"\" ).replace('\"',\"\" ).replace(\"''\", \"\").strip().split(',')\n",
    "        elif \"[\" in val:\n",
    "            val = val.replace('[',\"\" ).replace(']',\"\" ).replace(\"'\",\"\" ).strip().split(\",\")\n",
    "        return len(val)   \n",
    "    for col in count_cols:\n",
    "        df['count_'+col]= df[col].apply(to_apply)\n",
    "    return df\n",
    "########################\n",
    "def to_items(df, d_col): \n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    d_col -list of columns to divide the values to clean list of items\n",
    "    \n",
    "    OUTPUT\n",
    "    df - a dataframe with columns of choice cleaned and returns the values as lists\n",
    "    \"\"\"\n",
    "    def to_apply(val):\n",
    "        if \"{\" in val:\n",
    "            val = val.replace('}', \"\").replace('{', \"\").replace(\"'\",\"\" ).replace('\"',\"\" ).replace(\"''\", \"\").lower().split(',')\n",
    "        elif \"[\" in val:\n",
    "            val = val.replace('[',\"\" ).replace(']',\"\" ).replace(\"'\",\"\" ).lower().split(\",\")\n",
    "        return val  \n",
    "    def to_apply1(val):\n",
    "        new_val = []\n",
    "        if val == 'None':\n",
    "            new_val.append(val)\n",
    "        for i in list(val):\n",
    "            if (i != \"\") and ('translation' not in i.lower()):\n",
    "                new_val.append(i.strip())\n",
    "        return new_val\n",
    "    \n",
    "    def to_apply2(val):        \n",
    "        if 'None' in val:\n",
    "            return ['none']\n",
    "        elif len((val)) == 0:\n",
    "            return ['none']\n",
    "        else:\n",
    "            return list(val)\n",
    "    \n",
    "    for col in d_col:\n",
    "        df[col]= df[col].apply(to_apply)\n",
    "        df[col]= df[col].apply(to_apply1)\n",
    "        df[col]= df[col].apply(to_apply2)\n",
    "    return df\n",
    "def items_counter(df, d_col):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    count_col -list of columns to with lists as values to count\n",
    "    \n",
    "    OUTPUT\n",
    "    all_strings - a dictionary with the count of every value every list within every series\n",
    "    \"\"\"\n",
    "    all_strings= {}\n",
    "    def to_apply(val):\n",
    "        for i in val:\n",
    "                if i in list(all_strings.keys()):\n",
    "                    all_strings[i]+=1\n",
    "                else:\n",
    "                    all_strings[i]=1  \n",
    "\n",
    "    df[d_col].apply(to_apply)\n",
    "    return all_strings\n",
    "###################################\n",
    "def to_days(df, day_cols, na_date):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    day_cols -list of columns to divide the values to clean list of items\n",
    "    \n",
    "    OUTPUT\n",
    "    df - a dataframe with columns of choice cleaned and returns the values as lists\n",
    "    \"\"\"\n",
    "#Since Boston lisitngs span from September'16 to september'17, we can impute using the month of march'16\n",
    "#Since Seatle lisitngs span from January'16 to January'17, we can impute using the month of june'16\n",
    "    df = df.copy()\n",
    "    df[[day_cols[0], day_cols[1]]]=df[[day_cols[0], day_cols[1]]].apply(pd.to_datetime)\n",
    "    df = df.dropna(subset= [day_cols[0]], how ='any', axis = 0)\n",
    "    df[day_cols[1]] = df[day_cols[1]].fillna(pd.to_datetime(na_date))\n",
    "    df[day_cols[0]]= (df[day_cols[1]] - df[day_cols[0]]).apply(lambda x: round(x.value/(864*1e11)),2)\n",
    "    df= df.drop(day_cols[1], axis =1 )\n",
    "    df = df.reset_index(drop= True)\n",
    "    return df\n",
    "###########################################################################################################################   \n",
    "def applier(df1,df2,drop = True, float_=True, len_text= True, count= True, items = True,\n",
    "            parts = True , count_items = True, bool_num = True, days = True):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df1,df2 - 2 pandas dataframes\n",
    "    drop,float_,len_text, count, parts, date_time - Boolean values that corresponds to previosuly defined functions\n",
    "    OUTPUT\n",
    "    df - a clean dataframe that has undergone previously defined functions according to the boolean prameters passed\n",
    "    \"\"\"\n",
    "    while drop:\n",
    "        df1 = to_drop(df1, drop_cols)\n",
    "        df2 =to_drop(df2, drop_cols)\n",
    "        break\n",
    "    while float_:\n",
    "        df1 =to_float(df1, float_cols)\n",
    "        df2 =to_float(df2, float_cols)\n",
    "        break\n",
    "    while len_text:\n",
    "        df1, nltc = to_len_text(df1, len_text_cols)\n",
    "        df2, nltc = to_len_text(df2, len_text_cols)\n",
    "        break\n",
    "    while parts:\n",
    "        df1 = to_parts(df1, part_col)\n",
    "        df2 = to_parts(df2, part_col)\n",
    "        break\n",
    "    while count:\n",
    "        df1 = to_count(df1, count_cols)\n",
    "        df2 = to_count(df2, count_cols)\n",
    "        df1 = df1.drop('host_verifications', axis =1 )\n",
    "        df2 = df2.drop('host_verifications', axis =1 )        \n",
    "        break\n",
    "    while items:\n",
    "        df1 = to_items(df1, d_col)\n",
    "        df2 = to_items(df2, d_col)\n",
    "        break\n",
    "    while count_items:\n",
    "        b_amens_count = pd.Series(items_counter(df1,'amenities')).reset_index().rename(columns = {'index':'amenities', 0:'count'}).sort_values(by='count', ascending =False).reset_index(drop =True)\n",
    "        s_amens_count = pd.Series(items_counter(df2, 'amenities')).reset_index().rename(columns = {'index':'amenities', 0:'count'}).sort_values(by='count', ascending =False).reset_index(drop =True)\n",
    "        a_counts = [b_amens_count,s_amens_count]\n",
    "        break\n",
    "    while bool_num:\n",
    "        df1 = bool_nums(df1, bool_cols)\n",
    "        df2 = bool_nums(df2, bool_cols)\n",
    "        break\n",
    "    while days:\n",
    "        df1 = to_days(df1, day_cols, '2016-04-1')\n",
    "        df2 = to_days(df2, day_cols, '2016-06-1')\n",
    "        break\n",
    "    if count_items:\n",
    "        return df1, df2 ,a_counts\n",
    "    else:\n",
    "        return df1,df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "565c56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_list_1, s_list_1, a_counts = applier(b_list, s_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d53768",
   "metadata": {},
   "source": [
    "<font size = '3' >*Amenities seems like a good indicator of price as a response variable so let's have it dummified*<font/>\n",
    "<br>\n",
    "<font size = '2.75' >**This function takes forever(6 mins),so, it's commented out and I use the resulted dataframes that were written to CSV files**<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "64e5d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# def to_dummy(df1,df2, col1, cols_ref1,cols_ref2):\n",
    "    \n",
    "#     def construct(df,col, cols_ref):\n",
    "#         count = 0\n",
    "#         for val2 in df[col]:\n",
    "#             lister = []\n",
    "#             for val1 in cols_ref[col]:\n",
    "#                 if val1 in val2:\n",
    "#                     lister.append(1)\n",
    "#                 else:\n",
    "#                     lister.append(0)\n",
    "#             cols_ref = cols_ref.join(pd.Series(lister, name = count))\n",
    "#             count+=1\n",
    "#         cols_ref = cols_ref.drop('count', axis = 1).transpose()\n",
    "#         cols_ref.columns = list(cols_ref.iloc[0,:])\n",
    "#         return cols_ref\n",
    "#     b_amens_1  =construct(df1, col1,cols_ref1)\n",
    "#     s_amens_1  =construct(df2, col1,cols_ref2)\n",
    "#     b_amens_1 = b_amens_1.drop('none', axis = 1) #.drop(0,axis=0).reset_index(drop= True)\n",
    "#     b_amens_1 = b_amens_1.iloc[1:,:]\n",
    "#     b_amens_1.columns = [\"{}_{}\".format(col1,col) for col in b_amens_1.columns]\n",
    "#     s_amens_1 = s_amens_1.iloc[1:,:]\n",
    "#     s_amens_1 = s_amens_1.drop('none', axis = 1)\n",
    "#     s_amens_1.columns = [\"{}_{}\".format(col1,col) for col in s_amens_1.columns]\n",
    "#     b_dummies = b_amens_1.reset_index(drop =True)\n",
    "#     s_dummies = s_amens_1.reset_index(drop =True)\n",
    "#     df1 = df1.join(b_dummies)\n",
    "#     df2 = df2.join(s_dummies)\n",
    "#     df1 = df1.drop([col1], axis = 1)\n",
    "#     df2 = df2.drop([col1], axis = 1)\n",
    "#     return b_dummies, s_dummies, df1, df2\n",
    "    \n",
    "# b_d, s_d,b_list_d, s_list_d = to_dummy(b_list_1, s_list_1, 'amenities',\n",
    "#                        b_a_counts, s_a_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "36d9feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_list_d.to_csv('b_list_d.csv')\n",
    "# s_list_d.to_csv('s_list_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3591b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_list_d = pd.read_csv('b_list_d.csv', index_col = 0)\n",
    "s_list_d = pd.read_csv('s_list_d.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93351368",
   "metadata": {},
   "source": [
    "<font size = '3' >*Check the nulls again*<font/><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3c1b5b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>b_list_d_Nulls</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>nulls_proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host_location</td>\n",
       "      <td>0.306834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>host_response_time</td>\n",
       "      <td>13.138075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>host_response_rate</td>\n",
       "      <td>13.138075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>host_acceptance_rate</td>\n",
       "      <td>13.138075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>host_neighbourhood</td>\n",
       "      <td>9.456067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>city</td>\n",
       "      <td>0.055788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zipcode</td>\n",
       "      <td>1.059972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>market</td>\n",
       "      <td>0.390516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>property_type</td>\n",
       "      <td>0.083682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>0.390516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.278940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.251046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cleaning_fee</td>\n",
       "      <td>30.878661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>review_scores_rating</td>\n",
       "      <td>22.677824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>review_scores_accuracy</td>\n",
       "      <td>22.956764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>review_scores_cleanliness</td>\n",
       "      <td>22.817294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review_scores_checkin</td>\n",
       "      <td>22.873082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>22.817294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>22.928870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_scores_value</td>\n",
       "      <td>22.900976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>reviews_per_month</td>\n",
       "      <td>21.087866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>s_list_d_Nulls</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>nulls_proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host_location</td>\n",
       "      <td>0.157233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>host_response_time</td>\n",
       "      <td>13.653040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>host_response_rate</td>\n",
       "      <td>13.653040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>host_acceptance_rate</td>\n",
       "      <td>20.204403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>host_neighbourhood</td>\n",
       "      <td>7.809224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zipcode</td>\n",
       "      <td>0.183438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>property_type</td>\n",
       "      <td>0.026205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>0.419287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.157233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.026205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cleaning_fee</td>\n",
       "      <td>26.965409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>review_scores_rating</td>\n",
       "      <td>16.902516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>review_scores_accuracy</td>\n",
       "      <td>17.190776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>review_scores_cleanliness</td>\n",
       "      <td>17.059748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>review_scores_checkin</td>\n",
       "      <td>17.190776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>17.007338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>17.112159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review_scores_value</td>\n",
       "      <td>17.138365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>reviews_per_month</td>\n",
       "      <td>16.378407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1= (b_list_d.isnull().sum()[b_list_d.isnull().sum()>0]/b_list_d.shape[0]*100).reset_index().rename(columns ={'index':'col_name',0:'nulls_proportion'})\n",
    "df2 = (s_list_d.isnull().sum()[s_list_d.isnull().sum()>0]/s_list_d.shape[0]*100).reset_index().rename(columns ={'index':'col_name',0:'nulls_proportion'})\n",
    "display_side_by_side(df1,df2, titles =['b_list_d_Nulls','s_list_d_Nulls' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5135aa",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________\n",
    "### Comments:  \n",
    "[Boston & Seatle Listings]\n",
    "- Boston listings size : `3585`, `95`/ Seatle listings size : `3818`, `92`\n",
    "- Number of Non-null cols in Boston listings:  `51`, around half\n",
    "- Number of Non-null cols in Seatle listings:  `47`, around half<br>\n",
    "- I wrote a series of functions that commenced some basic cleaning to ease analysis, with the option to switch off any of them depending on the future requirements of the analyses, some of what was done:\n",
    ">- Columns with relatively high number nulls or that have little to no forseeable use were removed \n",
    ">- Took the charachter length of the values in some of the cols with long text entries and many unique values, possibly  the length of some fields maybe correlated somewhat with price.\n",
    ">- Columns with dates are transformed into Datetime, numerical values that were in text to floats\n",
    ">- Columns `amenities`and `host_verifications`were taken as counts, `amenities` was then dummified, for its seeming importance. \n",
    ">- `maximum_nights`column seems to lack some integrity so I divided it into time periods \n",
    "> Columns with t and f strings were converted into binary data. \n",
    ">- Difference between `host_since`and `last_review` was computed in days to `host_since`<br>\n",
    ">- All columns with only 't' or 'f' values were transformed in to binary values.\n",
    "\n",
    "- **After the basic cleaning and the dummification of `amenities`:** <br>\n",
    "~Boston listings size : `3585`, `98`/ Seatle listings size : `3818`, `98`. <br>\n",
    "~There are still nulls to deal with in case of modeling, but that depends on the requirements of each question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9abb4",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79126fde",
   "metadata": {},
   "source": [
    "### Step 1: Continue - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085b9a4",
   "metadata": {},
   "source": [
    "> **Boston & Seatle Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f748ba86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7202016</td>\n",
       "      <td>38917982</td>\n",
       "      <td>2015-07-19</td>\n",
       "      <td>28943674</td>\n",
       "      <td>Bianca</td>\n",
       "      <td>Cute and cozy place. Perfect location to every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39087409</td>\n",
       "      <td>2015-07-20</td>\n",
       "      <td>32440555</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Kelly has a great room in a very central locat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39820030</td>\n",
       "      <td>2015-07-26</td>\n",
       "      <td>37722850</td>\n",
       "      <td>Ian</td>\n",
       "      <td>Very spacious apartment, and in a great neighb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        id        date  reviewer_id reviewer_name  \\\n",
       "0     7202016  38917982  2015-07-19     28943674        Bianca   \n",
       "1     7202016  39087409  2015-07-20     32440555         Frank   \n",
       "2     7202016  39820030  2015-07-26     37722850           Ian   \n",
       "\n",
       "                                            comments  \n",
       "0  Cute and cozy place. Perfect location to every...  \n",
       "1  Kelly has a great room in a very central locat...  \n",
       "2  Very spacious apartment, and in a great neighb...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b_rev.head(3)\n",
    "s_rev.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2ef5d",
   "metadata": {},
   "source": [
    "<font size = '3' >*Check the sizes of cols & rows & check Nulls*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "972136a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston reviews size:  (68275 6)                          Seatle reviews size:  (84849 6)\n",
      "No. of unique listing ids:  2829                         No. of unique listing ids:  3191\n",
      "Number of Non-null cols in Boston Reviews:  5            Number of Non-null cols in Seatle Reviews:  5\n",
      "Null cols % in Boston:  comments    0.077627             Null cols % in Seatle:  comments    0.021214\n",
      "Null cols no. in Boston:  comments    53                 Null cols no. in Seatle:  comments    18\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_side_by_side(\"Boston reviews size:\", b_rev.shape,\"Seatle reviews size:\", s_rev.shape)\n",
    "print_side_by_side(\"No. of unique listing ids:\", b_rev.listing_id.unique().size,\"No. of unique listing ids:\", s_rev.listing_id.unique().size)\n",
    "print_side_by_side(\"Number of Non-null cols in Boston Reviews:\",  np.sum(b_rev.isnull().sum()==0), \n",
    "\"Number of Non-null cols in Seatle Reviews:\",  np.sum(s_rev.isnull().sum()==0))\n",
    "print_side_by_side(\"Null cols % in Boston:\", (b_rev.isnull().sum()[b_rev.isnull().sum()>0]/b_rev.shape[0]*100).to_string(),\n",
    "\"Null cols % in Seatle:\", (s_rev.isnull().sum()[s_rev.isnull().sum()>0]/s_rev.shape[0]*100).to_string())\n",
    "print_side_by_side(\"Null cols no. in Boston:\",(b_rev.isnull().sum()[b_rev.isnull().sum()>0]).to_string(),\n",
    "\"Null cols no. in Seatle:\", (s_rev.isnull().sum()[s_rev.isnull().sum()>0]).to_string())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719c299",
   "metadata": {},
   "source": [
    "<font size = '3' >**To extract analytical insights from the reviews entries, they ought to be transformed from text to numerical scores, to do so I will follow some steps:**<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d611bc",
   "metadata": {},
   "source": [
    "<font size = '3' >*1) Find all the words -excluding any non alphanumeric charachters - in each Dataset*<font/><br>\n",
    "<font size = '2' >**As the function takes 4 mins to execute, I commented it out and passed the resulted word lists as dfs to CSV files that were added to the project instead of running it in the notebook again.**<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "19f1d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# def get_words(df, col):\n",
    "#     \"\"\"\n",
    "#     INPUT\n",
    "#     df -pandas dataframe\n",
    "#     col -column of which the values are text \n",
    "#   \n",
    "#     OUTPUT\n",
    "#     df - a dataframe with a single colum of all the words \n",
    "#     \"\"\"\n",
    "#     all_strings = []\n",
    "#     for val in df[col]:\n",
    "#         try:\n",
    "#             val_strings = [''.join(filter(str.isalnum, i.lower())) for i in val.split() if len(i)>3]\n",
    "#         except:\n",
    "#             continue\n",
    "#         for word in val_strings:\n",
    "#             if word not in all_strings:\n",
    "#                 all_strings.append(word)\n",
    "#         val_strings = []\n",
    "#     return pd.Series(all_strings).to_frame().reset_index(drop = True).rename(columns = {0:'words'})\n",
    "# boston_words = get_words(b_rev, 'comments')\n",
    "# seatle_words = get_words(s_rev, 'comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "76dc36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boston_words.to_csv('boston_words.csv')\n",
    "# seatle_words.to_csv('seatle_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "689dfaae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston words no.:  54261\n",
      "Seatle words no.:  50627\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>Boston</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>islams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>Seatle</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cozy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boston_words = pd.read_csv('drafts/boston_words.csv', index_col= 0)\n",
    "seatle_words = pd.read_csv('drafts/seatle_words.csv', index_col= 0)\n",
    "print(\"Boston words no.: \", boston_words.shape[0])\n",
    "print(\"Seatle words no.: \", seatle_words.shape[0])\n",
    "display_side_by_side(boston_words.head(5), seatle_words.head(5), titles = [ 'Boston', 'Seatle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f9d14",
   "metadata": {},
   "source": [
    "<font size = '3' >*2) Read in positive and negative english word lists that are used for sentiment analysis*<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73810900",
   "metadata": {},
   "source": [
    "### Citation:\n",
    "* Using this resource  https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon I downloaded a list of words with positive and negative connotations used for sentiment analysis\n",
    "* *Based on the book*:  \n",
    "> Sentiment Analysis and Opinion Mining (Introduction and Survey), Morgan & Claypool, May 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6a1c368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words count:   2005                             Negative words count:   4781\n",
      "No. of positive words in Boston Reviews:   1147          No. of negative words in Boston Reviews:   1507\n",
      "No. of positive words in Seatle Reviews:   1235          No. of negative words in Seatle Reviews:   1556\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positive_words = pd.read_csv('drafts/positive-words.txt', sep = '\\t',encoding=\"ISO-8859-1\")\n",
    "negative_words = pd.read_csv('drafts/negative-words.txt', sep = '\\t',encoding=\"ISO-8859-1\")\n",
    "positive_words = positive_words.iloc[29:,:].reset_index(drop = True).rename(columns = {';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;':'words'})\n",
    "negative_words = negative_words.iloc[31:,:].reset_index(drop = True).rename(columns = {';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;':'words'})\n",
    "b_pos = np.intersect1d(np.array(boston_words['words'].astype(str)), np.array(positive_words['words']),assume_unique=True)\n",
    "b_neg = np.intersect1d(np.array(boston_words['words'].astype(str)), np.array(negative_words['words']),assume_unique=True)\n",
    "s_pos = np.intersect1d(np.array(seatle_words['words'].astype(str)), np.array(positive_words['words']),assume_unique=True)\n",
    "s_neg = np.intersect1d(np.array(seatle_words['words'].astype(str)), np.array(negative_words['words']),assume_unique=True)\n",
    "print_side_by_side('Positive words count: ', positive_words.shape[0]\n",
    ",'Negative words count: ', negative_words.shape[0])\n",
    "print_side_by_side(\"No. of positive words in Boston Reviews: \", len(b_pos)\n",
    ",\"No. of negative words in Boston Reviews: \", len(b_neg))\n",
    "print_side_by_side(\"No. of positive words in Seatle Reviews: \", len(s_pos)\n",
    ",\"No. of negative words in Seatle Reviews: \", len(s_neg))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dc5e4",
   "metadata": {},
   "source": [
    "<font size = '3' >*3) Let's translate the reviews from other languages to English*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6fe0c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dependency googletrans-4.0.0rc1\n",
    "##langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da6936",
   "metadata": {},
   "source": [
    "<font size='3'>*Let's drop the nulls, check the language of the reviews using `langdetect`, prepare the non english `comments` to be translated*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "05e5a1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of non English reviews in Boston:  0.05436662660138958\n",
      "Proportion of non English reviews in Seattle:  0.012424703233487757\n"
     ]
    }
   ],
   "source": [
    "# b_rev = b_rev.dropna(subset=['comments'], how = 'any', axis = 0)\n",
    "# s_rev = s_rev.dropna(subset=['comments'], how = 'any', axis = 0)\n",
    "\n",
    "# %%time\n",
    "# b_rev_t = b_rev.copy()\n",
    "# s_rev_t = s_rev.copy()\n",
    "# from langdetect import detect\n",
    "# def lang_check(val):\n",
    "#     try:\n",
    "#         return detect(val)\n",
    "#     except:\n",
    "#         return val\n",
    "    \n",
    "# b_rev_t['review_lang']=b_rev['comments'].apply(lang_check)\n",
    "# s_rev_t['review_lang']=s_rev['comments'].apply(lang_check)\n",
    "# b_rev_t.to_csv('b_rev_t.csv')\n",
    "# s_rev_t.to_csv('s_rev_t.csv')\n",
    "# b_rev_t = pd.read_csv('b_rev_t.csv', index_col = 0)\n",
    "s_rev_t = pd.read_csv('s_rev_t.csv', index_col = 0)\n",
    "# print('Proportion of non English reviews in Boston: ' ,b_rev_t[b_rev_t['review_lang']!= 'en'].shape[0]/b_rev_t.shape[0])\n",
    "# print('Proportion of non English reviews in Seattle: ',s_rev_t[s_rev_t['review_lang']!= 'en'].shape[0]/s_rev_t.shape[0])\n",
    "print(f\"\"\"Proportion of non English reviews in Boston:  0.05436662660138958\n",
    "Proportion of non English reviews in Seattle:  0.012424703233487757\"\"\")\n",
    "# b_to_trans =b_rev_t[b_rev_t['review_lang']!= 'en']\n",
    "# s_to_trans =s_rev_t[s_rev_t['review_lang']!= 'en']\n",
    "# b_to_trans['comments'] = b_to_trans['comments'].map(lambda val : str([re.sub(r\"[^a-zA-Z0-9]+\", '. ', k) for k in val.split(\"\\n\")]).replace('[',\" \").replace(']',\"\").replace(\"'\",\"\"))\n",
    "# s_to_trans['comments'] = s_to_trans['comments'].map(lambda val : str([re.sub(r\"[^a-zA-Z0-9]+\", '. ', k) for k in val.split(\"\\n\")]).replace('[',\" \").replace(']',\"\").replace(\"'\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf4047",
   "metadata": {},
   "source": [
    "<font size='3'>*Since googletrans library is extremely unstable, I break down the non-English reviews in Boston into 4 dataframes*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "025eabd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trans_slicer(df,df1 = 0,df2 = 0,df3 = 0, df4 = 0):\n",
    "#     dfs=[]\n",
    "#     for i in [df1,df2,df3,df4]:\n",
    "#         i = df[0:1000]\n",
    "#         df = df.drop(index = i.index.values,axis = 0).reset_index(drop= True)\n",
    "#         dfs.append(i.reset_index(drop =True))\n",
    "# #             df = df.drop(index = range(0,df.shape[0],1),axis = 0).reset_index(drop= True)\n",
    "#     return dfs\n",
    "# df1, df2, df3, df4 = trans_slicer(b_to_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "144c7b12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 9min 53s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# import re\n",
    "# import time\n",
    "# import googletrans\n",
    "# import httpx\n",
    "# from googletrans import Translator\n",
    "# timeout = httpx.Timeout(10) # 5 seconds timeout\n",
    "# translator = Translator(timeout=timeout)\n",
    "\n",
    "# def text_trans(val):\n",
    "#     vals = translator.translate(val, dest='en').text\n",
    "#     time.sleep(10)\n",
    "#     return vals\n",
    "# ############################################################\n",
    "# df1['t_comments'] = df2['comments'].apply(text_trans)\n",
    "# df1.to_csv('df2.csv')\n",
    "# df2['t_comments'] = df2['comments'].apply(text_trans)\n",
    "# df2.to_csv('df2.csv')\n",
    "# df3['t_comments'] = df3['comments'].apply(text_trans)\n",
    "# df3.to_csv('df3.csv')\n",
    "# df4['t_comments'] = df4['comments'].apply(text_trans)\n",
    "# df4.to_csv('df4.csv')\n",
    "# #4###########################################################\n",
    "# s_to_trans['t_comments'] = s_to_trans['comments'].apply(text_trans)\n",
    "# s_to_trans.to_csv('s_translate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "4d097a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = df1.append(df2)\n",
    "# dfs = dfs.append(df3)\n",
    "# dfs = dfs.append(df4)\n",
    "# dfs.index = b_to_trans.index\n",
    "# b_to_trans = dfs\n",
    "# b_to_trans['comments'] = b_to_trans['t_comments']\n",
    "# b_to_trans = b_to_trans.drop(columns =['t_comments'],axis = 1)\n",
    "#b_rev_t = b_rev_t.drop(index =b_to_trans.index,axis = 0)\n",
    "#b_rev_t = b_rev_t.append(b_to_trans)\n",
    "#b_rev_t = b_rev_t.sort_index(axis = 0).reset_index(drop= True)\n",
    "# b_rev_t['comments'] = b_rev_t['comments'].apply(lambda x: x.replace('.',' '))\n",
    "# b_rev_t.to_csv('b_rev_translated.csv')\n",
    "# s_to_trans['comments'] = s_to_trans['t_comments']\n",
    "# s_to_trans = s_to_trans.drop(columns =['t_comments'],axis = 1)\n",
    "# s_rev_t = s_rev_t.drop(index =s_to_trans.index,axis = 0)\n",
    "# s_rev_t = s_rev_t.append(s_to_trans)\n",
    "# s_rev_t = s_rev_t.sort_index(axis = 0).reset_index(drop= True)\n",
    "# s_rev_t['comments'] = s_rev_t['comments'].apply(lambda x: x.replace('.',' '))\n",
    "# s_rev_t.to_csv('s_rev_translated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40421f1b",
   "metadata": {},
   "source": [
    "<font size='3'>*Since googletrans takes around 3 hours to translate 1000 entries, that took some time, here are the resulted DataFrames*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "d5df8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_rev_trans = pd.read_csv('b_rev_translated.csv', index_col =0)\n",
    "s_rev_trans = pd.read_csv('s_rev_translated.csv', index_col =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cab414",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e15c5",
   "metadata": {},
   "source": [
    "<font size = '3' >*4) Add a scores column using the previous resource as a reference to evaulate the score of each review*<font/><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def create_scores(df,col, df_pos_array, df_neg_array):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    df -pandas dataframe\n",
    "    col -column with text reviews to be transformed in to positive and negative scores\n",
    "    pos_array- array with reference positive words for the passed df\n",
    "    neg_array- array with reference negative words for the passed df\n",
    "    OUTPUT\n",
    "    df - a dataframe with a score column containing positive and negative scores\"\n",
    "    \"\"\"\n",
    "    def get_score(val):\n",
    "        val_strings = [''.join(filter(str.isalnum, i.lower())) for i in str(val).split() if len(i)>3]\n",
    "        pos_score = len(np.intersect1d(np.array(val_strings).astype(object), df_pos_array, assume_unique =True))\n",
    "        neg_score = len(np.intersect1d(np.array(val_strings).astype(object), df_neg_array, assume_unique =True))\n",
    "        return pos_score - neg_score +1\n",
    "    df['score']= df[col].apply(get_score)\n",
    "    return df\n",
    "\n",
    "b_rev_score = create_scores(b_rev_trans, 'comments', b_pos, b_neg)\n",
    "s_rev_score = create_scores(s_rev_trans, 'comments', s_pos, s_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbdcc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_rev_score.to_csv('b_rev_score.csv')\n",
    "s_rev_score.to_csv('s_rev_score.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba243c62",
   "metadata": {},
   "source": [
    "<font size = '3' >*As this function takes a while as well, let's write the results into to csv files and read the frames again and then show some samples.*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d6c086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>Boston Reviews</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My stay at islam's place was really cool! Good location, 5min away from subway, then 10min from downtown. The room was nice, all place was clean. Islam managed pretty well our arrival, even if it was last minute ;) i do recommand this place to any airbnb user :)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great location for both airport and city - great amenities in the house: Plus Islam was always very helpful even though he was away</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We really enjoyed our stay at Islams house. From the outside the house didn't look so inviting but the inside was very nice! Even though Islam himself was not there everything was prepared for our arrival. The airport T Station is only a 5-10 min walk away. The only little issue was that all the people in the house had to share one bathroom. But it was not really a problem and it worked out fine. We would recommend Islams place for a stay in Boston.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>Seatle_reviews</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cute and cozy place. Perfect location to everything!</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kelly has a great room in a very central location. \\r\\nBeautiful building , architecture and a style that we really like. \\r\\nWe felt guite at home here and wish we had spent more time.\\r\\nWent for a walk and found Seattle Center with a major food festival in progress. What a treat.\\r\\nVisited the Space Needle and the Chihuly Glass exhibit. Then Pikes Place Market. WOW.  Thanks for a great stay.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very spacious apartment, and in a great neighborhood.  This is the kind of apartment I wish I had!\\r\\n\\r\\nDidn't really get to meet Kelly until I was on my out, but she was always readily available by phone. \\r\\n\\r\\nI believe the only \"issue\" (if you want to call it that) was finding a place to park, but I sincerely doubt its easy to park anywhere in a residential area after 5 pm on a Friday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b_rev_score = pd.read_csv('b_rev_score.csv', index_col = 0)\n",
    "s_rev_score = pd.read_csv('s_rev_score.csv', index_col = 0)\n",
    "sub_b_rev = b_rev_score.iloc[:,[5,6]]\n",
    "sub_s_rev = s_rev_score.iloc[:,[5,6]]\n",
    "display_side_by_side(sub_b_rev.head(3), sub_s_rev.head(3), titles= ['Boston Reviews', 'Seatle_reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f012e8b",
   "metadata": {},
   "source": [
    "<font size = '3' >*Let's display some descriptive statistics about the reviews table.*<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a2712bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum score in Boston :   33                           Maximum Score in Seatle :   38\n",
      "Minimum Score in Boston :   -17                          Minimum Score in Seatle :   -16\n",
      "Most common score in Boston:   0    4                    Most common score in Seatle:   0    4\n",
      "Mean score in Boston:   4.69                             Mean score in Seatle:   5.51\n",
      "Median common score in Boston:   4.0                     Median common score in Seatle:   5.0\n",
      "Standard deviation of score in Boston:   3.28            Standard deviation of score in Seatle:   3.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_side_by_side('Maximum score in Boston : ', b_rev_score.iloc[b_rev_score.score.idxmax()].score,\n",
    "                   'Maximum Score in Seatle : ', s_rev_score.iloc[s_rev_score.score.idxmax()].score)\n",
    "print_side_by_side('Minimum Score in Boston : ', b_rev_score.iloc[b_rev_score.score.idxmin()].score,\n",
    "                   'Minimum Score in Seatle : ', s_rev_score.iloc[s_rev_score.score.idxmin()].score)\n",
    "print_side_by_side('Most common score in Boston: ', b_rev_score['score'].mode().to_string(),\n",
    "'Most common score in Seatle: ', s_rev_score['score'].mode().to_string())\n",
    "print_side_by_side('Mean score in Boston: ', round(b_rev_score['score'].mean(),2)\n",
    ",'Mean score in Seatle: ', round(s_rev_score['score'].mean(),2))\n",
    "print_side_by_side('Median common score in Boston: ',round( b_rev_score['score'].median(),2),\n",
    "'Median common score in Seatle: ', s_rev_score['score'].median())\n",
    "print_side_by_side('Standard deviation of score in Boston: ', round(b_rev_score['score'].std(),2)\n",
    ",'Standard deviation of score in Seatle: ', round(s_rev_score['score'].std(),2))\n",
    "print('\\n')\n",
    "# print('Score: ', b_rev_score.iloc[b_rev_score.score.idxmax()].score)\n",
    "# b_rev_score.iloc[b_rev_score.score.idxmax()].comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be028fa",
   "metadata": {},
   "source": [
    " _______________________________________________________________________________________________________________________\n",
    "\n",
    "### Comments:  \n",
    "[Boston & Seatle Reviews]\n",
    "- Boston reviews size : (68275, 6)\n",
    "- Seatle reviews size : (84849, 6)\n",
    "- Nulls are only in `comments`columns in both Datasets: \n",
    "- Null percentage in Boston Reviews:  0.08%\n",
    "- Null percentage in Seatle Reviews: 0.02%\n",
    "- I added a score column to both tables to reflect positive or negative reviews numerically with the aid of an external resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991839b",
   "metadata": {},
   "source": [
    " _______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b572cd",
   "metadata": {},
   "source": [
    "### Step 2: Formulating Questions\n",
    "<font size = '3' >*After going through the data I think those questions would be of interest:*<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921fb8b1",
   "metadata": {},
   "source": [
    "### *Q: What aspects of a listing influences the price in both cities?*\n",
    "### *Q: How can we predict the price?*\n",
    "### *Q: How do prices vary through the year in both cities ? when is the season and off season in both cities?*\n",
    "### *Q:  How can you describe the reviews in each city ?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e85e41",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fb0d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# b_rev_score[b_rev_score['score']==0][['score', 'comments']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "905efd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "052c9f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (7,3))\n",
    "plt.hist(b_rev_score.score, bins = 25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995349d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d51257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a6f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
